{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f2d53d-d699-431e-8b4b-43ac8957ed47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ TensorFlow version: 2.20.0\n",
      "✅ Keras version: 3.11.3\n",
      "✅ GPU Available: []\n",
      "✅ Found 134 images in C:\\Users\\elroy\\OneDrive\\Documents\\glaucoma_detection\\glaucoma_dataset\\train\\glaucoma\\Glaucoma_Positive\n",
      "✅ Found 1020 images in C:\\Users\\elroy\\OneDrive\\Documents\\glaucoma_detection\\glaucoma_dataset\\train\\glaucoma\\Images - G1020\n",
      "✅ Found 650 images in C:\\Users\\elroy\\OneDrive\\Documents\\glaucoma_detection\\glaucoma_dataset\\train\\glaucoma\\Images - ORIGA\n",
      "✅ Found 400 images in C:\\Users\\elroy\\OneDrive\\Documents\\glaucoma_detection\\glaucoma_dataset\\train\\glaucoma\\Images - REFUGE\n",
      "✅ Total 2204 images found in C:\\Users\\elroy\\OneDrive\\Documents\\glaucoma_detection\\glaucoma_dataset\\train\\glaucoma\n",
      "✅ Found 2702 images in C:\\Users\\elroy\\OneDrive\\Documents\\glaucoma_detection\\glaucoma_dataset\\train\\normal\\Glaucoma_Negative\n",
      "✅ Found 2316 images in C:\\Users\\elroy\\OneDrive\\Documents\\glaucoma_detection\\glaucoma_dataset\\train\\normal\\Glaucoma_Negative_aug\n",
      "✅ Found 2 images in C:\\Users\\elroy\\OneDrive\\Documents\\glaucoma_detection\\glaucoma_dataset\\train\\normal\\Glaucoma_Negative_aug\\.ipynb_checkpoints\n",
      "✅ Total 5020 images found in C:\\Users\\elroy\\OneDrive\\Documents\\glaucoma_detection\\glaucoma_dataset\\train\\normal\n",
      "✅ Found 34 images in C:\\Users\\elroy\\OneDrive\\Documents\\glaucoma_detection\\glaucoma_dataset\\val\\glaucoma\\Glaucoma_Positive\n",
      "✅ Found 40 images in C:\\Users\\elroy\\OneDrive\\Documents\\glaucoma_detection\\glaucoma_dataset\\val\\glaucoma\\Images\n",
      "✅ Total 74 images found in C:\\Users\\elroy\\OneDrive\\Documents\\glaucoma_detection\\glaucoma_dataset\\val\\glaucoma\n",
      "✅ Found 96 images in C:\\Users\\elroy\\OneDrive\\Documents\\glaucoma_detection\\glaucoma_dataset\\val\\normal\\Glaucoma_Negative\n",
      "✅ Total 96 images found in C:\\Users\\elroy\\OneDrive\\Documents\\glaucoma_detection\\glaucoma_dataset\\val\\normal\n",
      "✅ Found 400 images in C:\\Users\\elroy\\OneDrive\\Documents\\glaucoma_detection\\glaucoma_dataset\\test\\glaucoma\\Images\n",
      "✅ Total 400 images found in C:\\Users\\elroy\\OneDrive\\Documents\\glaucoma_detection\\glaucoma_dataset\\test\\glaucoma\n",
      "✅ Found 37 images in C:\\Users\\elroy\\OneDrive\\Documents\\glaucoma_detection\\glaucoma_dataset\\test\\normal\\Glaucoma_Negative\n",
      "✅ Total 37 images found in C:\\Users\\elroy\\OneDrive\\Documents\\glaucoma_detection\\glaucoma_dataset\\test\\normal\n",
      "✅ Actual Training - Glaucoma: 2204, Normal: 5020\n",
      "📊 Adjusted Class Weights: {0: 0.7195219123505976, 1: 1.6388384754990926}\n",
      "Found 7224 validated image filenames.\n",
      "Found 170 validated image filenames.\n",
      "Found 437 validated image filenames.\n",
      "Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb4_notop.h5\n",
      "⚠️ Warning: Failed to load pre-trained weights due to URL fetch failure on https://storage.googleapis.com/keras-applications/efficientnetb4_notop.h5: None -- [Errno 11001] getaddrinfo failed. Proceeding without them.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ efficientnetb4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1792</span>)     │    <span style=\"color: #00af00; text-decoration-color: #00af00\">17,673,823</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1792</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1792</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">459,008</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ efficientnetb4 (\u001b[38;5;33mFunctional\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m1792\u001b[0m)     │    \u001b[38;5;34m17,673,823\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1792\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1792\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │       \u001b[38;5;34m459,008\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │         \u001b[38;5;34m1,024\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m32,896\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │           \u001b[38;5;34m129\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">18,166,880</span> (69.30 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m18,166,880\u001b[0m (69.30 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">492,545</span> (1.88 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m492,545\u001b[0m (1.88 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">17,674,335</span> (67.42 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m17,674,335\u001b[0m (67.42 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Phase 1: Training top layers...\n",
      "Epoch 1/50\n",
      "\u001b[1m 14/452\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m6:17\u001b[0m 863ms/step - accuracy: 0.6628 - auc: 0.4999 - loss: 0.0829 - precision: 0.3636 - recall: 0.1911"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import shutil\n",
    "import random\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Focal Loss implementation for imbalance\n",
    "def focal_loss(gamma=2.0, alpha=0.25):\n",
    "    def focal_loss_fixed(y_true, y_pred):\n",
    "        epsilon = tf.keras.backend.epsilon()\n",
    "        y_pred = tf.clip_by_value(y_pred, epsilon, 1. - epsilon)\n",
    "        pt = tf.where(tf.equal(y_true, 1), y_pred, 1 - y_pred)\n",
    "        alpha_t = tf.where(tf.equal(y_true, 1), alpha, 1 - alpha)\n",
    "        return -alpha_t * tf.pow(1. - pt, gamma) * tf.math.log(pt)  # Fixed to tf.math.log\n",
    "    return focal_loss_fixed\n",
    "\n",
    "# Clear Keras cache\n",
    "cache_dir = os.path.expanduser(\"~/.keras/models\")\n",
    "if os.path.exists(cache_dir):\n",
    "    for file in os.listdir(cache_dir):\n",
    "        if \"efficientnet\" in file.lower():\n",
    "            os.remove(os.path.join(cache_dir, file))\n",
    "            print(f\"🧹 Cleared cached file: {file}\")\n",
    "\n",
    "print(\"✅ TensorFlow version:\", tf.__version__)\n",
    "print(\"✅ Keras version:\", keras.__version__)\n",
    "print(\"✅ GPU Available:\", tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "# 📁 Define paths\n",
    "DATASET_ROOT = \"C:\\\\Users\\\\elroy\\\\OneDrive\\\\Documents\\\\glaucoma_detection\\\\glaucoma_dataset\"\n",
    "TRAIN_DIR = os.path.join(DATASET_ROOT, \"train\")\n",
    "VAL_DIR = os.path.join(DATASET_ROOT, \"val\")\n",
    "TEST_DIR = os.path.join(DATASET_ROOT, \"test\")\n",
    "\n",
    "# 🧪 Validate dataset\n",
    "def validate_dataset(directory):\n",
    "    if not os.path.exists(directory):\n",
    "        raise FileNotFoundError(f\"Directory not found: {directory}\")\n",
    "    for class_name in ['glaucoma', 'normal']:\n",
    "        class_dir = os.path.join(directory, class_name)\n",
    "        if not os.path.exists(class_dir):\n",
    "            raise FileNotFoundError(f\"Class directory not found: {class_dir}\")\n",
    "        total_images = 0\n",
    "        for root, _, files in os.walk(class_dir):\n",
    "            images = [f for f in files if f.lower().endswith(('.png', '.jpg', '.jpeg', '.tiff', '.bmp'))]\n",
    "            total_images += len(images)\n",
    "            if images:\n",
    "                print(f\"✅ Found {len(images)} images in {root}\")\n",
    "        if total_images == 0:\n",
    "            raise FileNotFoundError(f\"No valid images found under {class_dir}\")\n",
    "        print(f\"✅ Total {total_images} images found in {class_dir}\")\n",
    "\n",
    "validate_dataset(TRAIN_DIR)\n",
    "validate_dataset(VAL_DIR)\n",
    "validate_dataset(TEST_DIR)\n",
    "\n",
    "# 🖼️ Preprocessing\n",
    "def preprocess_image(img_array):\n",
    "    if img_array is None:\n",
    "        raise ValueError(\"Input image is None.\")\n",
    "    img_array = cv2.resize(img_array, (224, 224))\n",
    "    if len(img_array.shape) == 2:\n",
    "        img_array = np.stack([img_array] * 3, axis=-1)\n",
    "    elif img_array.shape[-1] == 1:\n",
    "        img_array = np.concatenate([img_array] * 3, axis=-1)\n",
    "    elif img_array.shape[-1] == 4:\n",
    "        img_array = img_array[:, :, :3]\n",
    "    elif img_array.shape[-1] != 3:\n",
    "        raise ValueError(f\"Unsupported channel count: {img_array.shape[-1]}\")\n",
    "    if img_array.dtype != np.uint8:\n",
    "        if img_array.max() <= 1.0:\n",
    "            img_array = (img_array * 255).astype(np.uint8)\n",
    "        else:\n",
    "            img_array = img_array.astype(np.uint8)\n",
    "    lab = cv2.cvtColor(img_array, cv2.COLOR_RGB2LAB)\n",
    "    l, a, b = cv2.split(lab)\n",
    "    clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8,8))\n",
    "    cl = clahe.apply(l)\n",
    "    limg = cv2.merge((cl, a, b))\n",
    "    img_array = cv2.cvtColor(limg, cv2.COLOR_LAB2RGB)\n",
    "    return img_array.astype(np.float32) / 255.0\n",
    "\n",
    "# Build DataFrames with augmented images\n",
    "def build_image_dataframe(base_dir):\n",
    "    data = []\n",
    "    for class_name in ['glaucoma', 'normal']:\n",
    "        class_path = os.path.join(base_dir, class_name)\n",
    "        for root, _, files in os.walk(class_path):\n",
    "            for file in files:\n",
    "                if file.lower().endswith(('.png', '.jpg', '.jpeg', '.tiff', '.bmp')):\n",
    "                    data.append([os.path.join(root, file), 1 if class_name == 'glaucoma' else 0])\n",
    "    return pd.DataFrame(data, columns=['filename', 'class'])\n",
    "\n",
    "train_df = build_image_dataframe(TRAIN_DIR)\n",
    "val_df = build_image_dataframe(VAL_DIR)\n",
    "test_df = build_image_dataframe(TEST_DIR)\n",
    "\n",
    "glaucoma_train_count = len(train_df[train_df['class'] == 1])\n",
    "normal_train_count = len(train_df[train_df['class'] == 0])\n",
    "print(f\"✅ Actual Training - Glaucoma: {glaucoma_train_count}, Normal: {normal_train_count}\")\n",
    "\n",
    "total_train = glaucoma_train_count + normal_train_count\n",
    "class_weights = {\n",
    "    0: total_train / (2 * normal_train_count),  # Dynamic weight for normal\n",
    "    1: total_train / (2 * glaucoma_train_count)  # Dynamic weight for glaucoma\n",
    "}\n",
    "print(\"📊 Adjusted Class Weights:\", class_weights)\n",
    "\n",
    "# 🌱 Data Generators with Aggressive Augmentation\n",
    "IMG_SIZE = (224, 224)\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_image,\n",
    "    rotation_range=30,\n",
    "    width_shift_range=0.3,\n",
    "    height_shift_range=0.3,\n",
    "    shear_range=0.3,\n",
    "    zoom_range=0.3,\n",
    "    horizontal_flip=True,\n",
    "    brightness_range=[0.7, 1.3],\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "train_generator = train_datagen.flow_from_dataframe(\n",
    "    train_df,\n",
    "    x_col='filename',\n",
    "    y_col='class',\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='raw',\n",
    "    shuffle=True,\n",
    "    seed=42,\n",
    "    color_mode='rgb'\n",
    ")\n",
    "\n",
    "val_datagen = ImageDataGenerator(preprocessing_function=preprocess_image)\n",
    "val_generator = val_datagen.flow_from_dataframe(\n",
    "    val_df,\n",
    "    x_col='filename',\n",
    "    y_col='class',\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='raw',\n",
    "    shuffle=False,\n",
    "    seed=42,\n",
    "    color_mode='rgb'\n",
    ")\n",
    "\n",
    "test_datagen = ImageDataGenerator(preprocessing_function=preprocess_image)\n",
    "test_generator = test_datagen.flow_from_dataframe(\n",
    "    test_df,\n",
    "    x_col='filename',\n",
    "    y_col='class',\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='raw',\n",
    "    shuffle=False,\n",
    "    seed=42,\n",
    "    color_mode='rgb'\n",
    ")\n",
    "\n",
    "# Model with Focal Loss\n",
    "input_shape = (224, 224, 3)\n",
    "base_model = tf.keras.applications.EfficientNetB4(\n",
    "    weights=None,\n",
    "    include_top=False,\n",
    "    input_shape=input_shape\n",
    ")\n",
    "\n",
    "try:\n",
    "    weights_path = tf.keras.utils.get_file(\n",
    "        'efficientnetb4_notop.h5',\n",
    "        'https://storage.googleapis.com/keras-applications/efficientnetb4_notop.h5',\n",
    "        cache_subdir='models',\n",
    "        file_hash=None\n",
    "    )\n",
    "    base_model.load_weights(weights_path, by_name=True, skip_mismatch=True)\n",
    "    print(\"✅ Pre-trained weights loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠️ Warning: Failed to load pre-trained weights due to {e}. Proceeding without them.\")\n",
    "\n",
    "base_model.trainable = False\n",
    "\n",
    "model = models.Sequential([\n",
    "    base_model,\n",
    "    layers.GlobalAveragePooling2D(),\n",
    "    layers.Dropout(0.6),\n",
    "    layers.Dense(256, activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.4),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optimizers.Adam(learning_rate=1e-3),\n",
    "    loss=focal_loss(gamma=2.0, alpha=0.25),\n",
    "    metrics=['accuracy', 'precision', 'recall', 'auc']\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Callbacks\n",
    "checkpoint = ModelCheckpoint(\n",
    "    'best_glaucoma_model.h5',\n",
    "    monitor='val_accuracy',\n",
    "    save_best_only=True,\n",
    "    mode='max',\n",
    "    verbose=1\n",
    ")\n",
    "early_stop = EarlyStopping(\n",
    "    monitor='val_auc',\n",
    "    patience=10,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,\n",
    "    patience=5,\n",
    "    min_lr=1e-6,\n",
    "    verbose=1\n",
    ")\n",
    "callbacks = [checkpoint, early_stop, reduce_lr]\n",
    "\n",
    "# 🚀 Phase 1: Train top layers\n",
    "print(\"🚀 Phase 1: Training top layers...\")\n",
    "history1 = model.fit(\n",
    "    train_generator,\n",
    "    epochs=50,\n",
    "    validation_data=val_generator,\n",
    "    class_weight=class_weights,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# 🔥 Phase 2: Fine-tune\n",
    "print(\"🔥 Phase 2: Fine-tuning last 50 layers...\")\n",
    "base_model.trainable = True\n",
    "for layer in base_model.layers[:-50]:\n",
    "    layer.trainable = False\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optimizers.Adam(learning_rate=1e-4),\n",
    "    loss=focal_loss(gamma=2.0, alpha=0.25),\n",
    "    metrics=['accuracy', 'precision', 'recall', 'auc']\n",
    ")\n",
    "\n",
    "history2 = model.fit(\n",
    "    train_generator,\n",
    "    epochs=50,\n",
    "    validation_data=val_generator,\n",
    "    class_weight=class_weights,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# 📊 Plot Training History\n",
    "def plot_training_history(history1, history2=None):\n",
    "    histories = [history1]\n",
    "    if history2:\n",
    "        histories.append(history2)\n",
    "    combined_hist = {}\n",
    "    for key in histories[0].history.keys():\n",
    "        combined_hist[key] = []\n",
    "        for h in histories:\n",
    "            combined_hist[key].extend(h.history[key])\n",
    "    epochs = range(1, len(combined_hist['loss']) + 1)\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    axes = axes.flatten()\n",
    "    for i, metric in enumerate(['accuracy', 'loss', 'precision', 'recall']):\n",
    "        axes[i].plot(epochs, combined_hist[metric], label=f'Training {metric}')\n",
    "        axes[i].plot(epochs, combined_hist[f'val_{metric}'], label=f'Validation {metric}')\n",
    "        axes[i].set_title(f'{metric.capitalize()}')\n",
    "        axes[i].legend()\n",
    "        axes[i].grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_training_history(history1, history2)\n",
    "\n",
    "# 🧪 Evaluate on Test Set with Threshold Tuning\n",
    "print(\"🧪 Evaluating on Test Set...\")\n",
    "test_results = model.evaluate(test_generator, verbose=1)\n",
    "print(f\"Default Test Accuracy: {test_results[1]:.4f}\")\n",
    "\n",
    "y_true = test_generator.classes\n",
    "y_pred_prob = model.predict(test_generator, verbose=1).flatten()\n",
    "thresholds = np.arange(0.1, 0.9, 0.05)\n",
    "best_thresh = 0.5\n",
    "best_acc = test_results[1]\n",
    "for thresh in thresholds:\n",
    "    y_pred_thresh = (y_pred_prob > thresh).astype(int)\n",
    "    acc = accuracy_score(y_true, y_pred_thresh)\n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "        best_thresh = thresh\n",
    "print(f\"Best Threshold: {best_thresh:.2f}, Best Accuracy: {best_acc:.4f}\")\n",
    "\n",
    "y_pred_best = (y_pred_prob > best_thresh).astype(int)\n",
    "print(\"\\n📋 Classification Report (Best Threshold):\")\n",
    "print(classification_report(y_true, y_pred_best, target_names=['Normal', 'Glaucoma']))\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred_best)\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Normal', 'Glaucoma'], yticklabels=['Normal', 'Glaucoma'])\n",
    "plt.title('Confusion Matrix (Best Threshold)')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()\n",
    "\n",
    "model.save('final_glaucoma_model.h5')\n",
    "print(\"💾 Model saved as 'final_glaucoma_model.h5'\")\n",
    "\n",
    "print(\"\\n🎉 FINAL MODEL PERFORMANCE (Best Threshold):\")\n",
    "print(f\"Test Accuracy: {best_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88db76a1-f70e-4e98-924f-d38bd0c33723",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Generated 2316 augmented normal images in C:\\Users\\elroy\\OneDrive\\Documents\\glaucoma_detection\\glaucoma_dataset\\train\\normal\\Glaucoma_Negative_aug\n",
      "✅ Copied augmented images to original normal folder. Re-run training now!\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Paths\n",
    "normal_dir = r\"C:\\Users\\elroy\\OneDrive\\Documents\\glaucoma_detection\\glaucoma_dataset\\train\\normal\\Glaucoma_Negative\"\n",
    "augmented_dir = r\"C:\\Users\\elroy\\OneDrive\\Documents\\glaucoma_detection\\glaucoma_dataset\\train\\normal\\Glaucoma_Negative_aug\"\n",
    "os.makedirs(augmented_dir, exist_ok=True)\n",
    "\n",
    "# Simple augmentation functions\n",
    "def rotate_image(img, angle):\n",
    "    h, w = img.shape[:2]\n",
    "    center = (w // 2, h // 2)\n",
    "    rot_matrix = cv2.getRotationMatrix2D(center, angle, 1.0)\n",
    "    return cv2.warpAffine(img, rot_matrix, (w, h), borderMode=cv2.BORDER_REFLECT)\n",
    "\n",
    "def add_noise(img):\n",
    "    mean = 0\n",
    "    var = np.random.randint(10, 50)\n",
    "    sigma = var ** 0.5\n",
    "    gaussian = np.random.normal(mean, sigma, img.shape)\n",
    "    noisy = img.astype(np.float32) + gaussian\n",
    "    return np.clip(noisy, 0, 255).astype(np.uint8)\n",
    "\n",
    "def adjust_brightness_contrast(img, alpha=1.0, beta=10):\n",
    "    adjusted = cv2.convertScaleAbs(img, alpha=alpha, beta=beta)\n",
    "    return adjusted\n",
    "\n",
    "# Load original images\n",
    "original_images = [f for f in os.listdir(normal_dir) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "\n",
    "for img_file in original_images:  # Removed tqdm\n",
    "    img_path = os.path.join(normal_dir, img_file)\n",
    "    img = cv2.imread(img_path)\n",
    "    if img is None:\n",
    "        continue\n",
    "\n",
    "    # Generate 3 augmentations per image\n",
    "    augmentations = [\n",
    "        rotate_image(img, np.random.uniform(-15, 15)),  # Rotation\n",
    "        cv2.flip(img, 1),  # Horizontal flip\n",
    "        adjust_brightness_contrast(img, np.random.uniform(0.9, 1.1), np.random.uniform(-10, 10)),  # Brightness/contrast\n",
    "        add_noise(img),  # Noise\n",
    "        rotate_image(cv2.flip(img, 1), np.random.uniform(-10, 10)),  # Combined\n",
    "        adjust_brightness_contrast(add_noise(img), np.random.uniform(0.8, 1.2), np.random.uniform(-15, 15))  # Combined\n",
    "    ]\n",
    "\n",
    "    for i, aug_img in enumerate(augmentations):\n",
    "        aug_filename = f\"{os.path.splitext(img_file)[0]}_aug_{i}.jpg\"\n",
    "        cv2.imwrite(os.path.join(augmented_dir, aug_filename), aug_img)\n",
    "\n",
    "print(f\"✅ Generated {len(original_images) * 6} augmented normal images in {augmented_dir}\")\n",
    "\n",
    "# Copy augmented images to original folder for training\n",
    "for aug_file in os.listdir(augmented_dir):\n",
    "    shutil.copy(os.path.join(augmented_dir, aug_file), os.path.join(normal_dir, aug_file))\n",
    "print(\"✅ Copied augmented images to original normal folder. Re-run training now!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "507da6c5-fc11-4888-bb12-4421c182728b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras Version: 3.11.3\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "gpu_list = tf.config.list_physical_devices('GPU')\n",
    "\n",
    "if gpu_list:\n",
    "    print(\"GPU is detected! ✅\")\n",
    "    for gpu in gpu_list:\n",
    "        print(f\"- {gpu.name}\")\n",
    "else:\n",
    "    print(\"!!! GPU NOT DETECTED !!! ❌\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2acd31bf-ea0e-4943-9ad2-ce4e1989243c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a295cfe-a7f3-4e86-9318-a8632141a62d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (glaucoma_env)",
   "language": "python",
   "name": "glaucoma_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
